    import os
    import sys
    import torch
    import lmdb
    import pickle
    import numpy as np
    from pathlib import Path
    from tqdm import tqdm
    from torch.utils.data import DataLoader
    from transformers import AutoModel
    import pandas as pd

    # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from src.utils.dataset import BindingLMDBDataset


    # --- –ù–ê–°–¢–†–û–ô–ö–ò ---
    INFERENCE_BATCH_SIZE = 32  
    NUM_WORKERS = 4 
    COMMIT_EVERY = 1000  # ‚Üê –ö–õ–Æ–ß–ï–í–ê–Ø –ù–ê–°–¢–†–û–ô–ö–ê: –∫–æ–º–º–∏—Ç–∏–º –∫–∞–∂–¥—ã–µ N –∑–∞–ø–∏—Å–µ–π


    class IdAwareDataset(BindingLMDBDataset):
        """
        –û–±–µ—Ä—Ç–∫–∞, –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç row_id –≤–º–µ—Å—Ç–µ —Å –¥–∞–Ω–Ω—ã–º–∏.
        """
        def __getitem__(self, idx):
            data = super().__getitem__(idx)
            # –î–æ–±–∞–≤–ª—è–µ–º ID —Ç–µ–∫—É—â–µ–π –∑–∞–ø–∏—Å–∏
            data['row_id'] = self.indices[idx]
            return data


    def main():
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"üöÄ Device: {device}")

        # 1. –ü—É—Ç–∏
        script_dir = Path(__file__).resolve().parent
        project_root = script_dir.parent
        
        base_data_dir = Path("../data/baseline") 
        input_db_path = base_data_dir / "db_processed"
        output_db_path = base_data_dir / "db_embeddings"
        index_path = base_data_dir / "index.csv"
        
        models_dir = Path("../models/pretrained")
        prot_path = models_dir / "protbert"
        mol_path = models_dir / "chemberta"

        print(f"üìÇ Input: {input_db_path}")
        print(f"üíæ Output: {output_db_path}")

        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—ã—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        output_db_path.parent.mkdir(parents=True, exist_ok=True)

        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
        print("üß† Loading Models...")
        prot_bert = AutoModel.from_pretrained(str(prot_path)).to(device).eval()
        mol_bert = AutoModel.from_pretrained(str(mol_path)).to(device).eval()
        
        # 3. –î–∞—Ç–∞—Å–µ—Ç
        print("üìä Loading Dataset...")
        full_dataset = IdAwareDataset(str(input_db_path), str(index_path))
        total_records = len(full_dataset)
        print(f"   Total records: {total_records}")
        
        loader = DataLoader(
            full_dataset, 
            batch_size=INFERENCE_BATCH_SIZE, 
            shuffle=False, 
            num_workers=NUM_WORKERS,
            pin_memory=True
        )

        # 4. LMDB –æ–∫—Ä—É–∂–µ–Ω–∏–µ
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫–∞—Ä—Ç—ã –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        map_size = 100 * 1024 * 1024 * 1024  # 100 GB
        
        print(f"üîê Opening LMDB: {output_db_path}")
        env = lmdb.open(str(output_db_path), map_size=map_size)
        
        print(f"üî• Starting Encoding... Batch Size: {INFERENCE_BATCH_SIZE}, Commit Every: {COMMIT_EVERY}")
        
        count = 0
        batch_count = 0
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(loader, desc="Encoding", total=len(loader))):
                try:
                    # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ GPU
                    mol_input = {k: v.to(device) for k, v in batch['mol_input'].items()}
                    prot_input = {k: v.to(device) for k, v in batch['prot_input'].items()}
                    
                    # AMP Autocast (FP16)
                    with torch.cuda.amp.autocast():
                        # ProtBERT
                        prot_out = prot_bert(**prot_input)
                        p_mask = prot_input['attention_mask'].unsqueeze(-1).expand(prot_out.last_hidden_state.size()).float()
                        p_emb = torch.sum(prot_out.last_hidden_state * p_mask, 1) / torch.clamp(p_mask.sum(1), min=1e-9)

                        # ChemBERTa
                        mol_out = mol_bert(**mol_input)
                        m_mask = mol_input['attention_mask'].unsqueeze(-1).expand(mol_out.last_hidden_state.size()).float()
                        m_emb = torch.sum(mol_out.last_hidden_state * m_mask, 1) / torch.clamp(m_mask.sum(1), min=1e-9)

                        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ (–Ω–∞ CPU)
                        combined = torch.cat((p_emb, m_emb), dim=1).cpu().numpy().astype(np.float32)
                    
                    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                    targets = batch['targets'].numpy().astype(np.float32)
                    row_ids = batch['row_id']  # –¢–µ–Ω–∑–æ—Ä —Å ID –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞

                    # ‚úÖ –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º txn –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞ –æ—Ç–¥–µ–ª—å–Ω–æ
                    txn = env.begin(write=True)
                    try:
                        for i, rid in enumerate(row_ids):
                            save_key = f"emb_{rid}".encode()
                            save_obj = {
                                'embedding': combined[i], 
                                'target': targets[i]
                            }
                            txn.put(save_key, pickle.dumps(save_obj))
                            count += 1
                        
                        # ‚úÖ –ö–æ–º–º–∏—Ç–∏–º –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞
                        txn.commit()
                        batch_count += 1
                        
                    except Exception as e:
                        print(f"\n‚ö†Ô∏è  Error in batch {batch_idx}: {e}")
                        txn.abort()
                        raise e

                    # –í—ã–≤–æ–¥–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –±–∞—Ç—á–µ–π
                    if batch_count % 10 == 0:
                        print(f"   Progress: {count}/{total_records} ({100*count/total_records:.1f}%)")
                        
                except RuntimeError as e:
                    if "out of memory" in str(e):
                        print("\n‚ùå CUDA Out of Memory!")
                        print(f"   Processed {count} records before OOM.")
                        torch.cuda.empty_cache()
                        sys.exit(1)
                    else:
                        raise e

        # –ó–∞–∫—Ä—ã—Ç–∏–µ LMDB
        env.close()
        print(f"\n‚úÖ SUCCESS! Processed {count} records.")
        print(f"üìÅ Embeddings saved to: {output_db_path}")


    if __name__ == "__main__":
        main()