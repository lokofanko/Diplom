import sys
import os
import yaml
import lmdb
import pickle
import pandas as pd
import numpy as np
from tqdm import tqdm
from pathlib import Path

# –ü—É—Ç–∏
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.utils.tokenizers import MolProteinTokenizer

def main(config_path):
    print("üöÄ –ó–ê–ü–£–°–ö –ü–†–ï–ü–†–û–¶–ï–°–°–ò–ù–ì–ê –î–ê–ù–ù–´–•")
    
    with open(config_path, 'r') as f:
        cfg = yaml.safe_load(f)

    raw_db_path = cfg['paths']['db_path']
    processed_db_path = cfg['paths']['processed_db_path']
    index_path = os.path.join(os.path.dirname(raw_db_path), 'index.csv')

    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –Ω–æ–≤–æ–π –±–∞–∑—ã
    os.makedirs(processed_db_path, exist_ok=True)

    print("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ (—ç—Ç–æ –∑–∞–π–º–µ—Ç –ø–∞—Ä—É —Å–µ–∫—É–Ω–¥)...")
    tokenizer = MolProteinTokenizer(
        mol_model_path=cfg['paths']['mol_model_path'],
        prot_model_path=cfg['paths']['prot_model_path'],
        mol_max_len=cfg['data']['mol_max_len'],
        prot_max_len=cfg['data']['prot_max_len']
    )

    print("üìñ –ß—Ç–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞...")
    df = pd.read_csv(index_path)
    print(f"   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df):,}")

    # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Å—ã—Ä—É—é –±–∞–∑—É –Ω–∞ —á—Ç–µ–Ω–∏–µ
    env_src = lmdb.open(raw_db_path, readonly=True, lock=False)
    
    # –û—Ç–∫—Ä—ã–≤–∞–µ–º –Ω–æ–≤—É—é –±–∞–∑—É –Ω–∞ –∑–∞–ø–∏—Å—å (—Ä–∞–∑–º–µ—Ä —Å –∑–∞–ø–∞—Å–æ–º 50–ì–ë)
    env_dst = lmdb.open(processed_db_path, map_size=int(50e9))

    print("‚öôÔ∏è  –ù–∞—á–∏–Ω–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∏ –∑–∞–ø–∏—Å—å...")
    
    batch_size = 1000
    cache = []
    
    with env_src.begin() as txn_src, env_dst.begin(write=True) as txn_dst:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º tqdm –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="Processing"):
            row_id = row['row_id']
            
            # –ß–∏—Ç–∞–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
            raw_bytes = txn_src.get(f'sample_{row_id}'.encode())
            if not raw_bytes: continue
            
            sample = pickle.loads(raw_bytes)
            smiles = sample['smiles']
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É—Å—Ç–æ–≥–æ –±–µ–ª–∫–∞ (–¥–ª—è HiQBind)
            prot_seq = sample['protein_sequence'] if sample['protein_sequence'] else ""
            
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º (–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–Ω–∑–æ—Ä—ã)
            mol_out, prot_out = tokenizer.tokenize_single(smiles, prot_seq)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–Ω–∑–æ—Ä—ã –≤ numpy (—á—Ç–æ–±—ã –º–µ–Ω—å—à–µ –≤–µ—Å–∏–ª–∏ –≤ pickle)
            # –ú–æ–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –∏ torch tensor, –Ω–æ pickle numpy —á–∞—Å—Ç–æ –±—ã—Å—Ç—Ä–µ–µ
            processed_record = {
                'mol_input': {k: v.numpy() for k,v in mol_out.items()},
                'prot_input': {k: v.numpy() for k,v in prot_out.items()},
                'targets': float(sample['pIC50']),
                'row_id': row_id
            }
            
            # –ü–∏—à–µ–º –≤ –Ω–æ–≤—É—é –±–∞–∑—É
            txn_dst.put(f'proc_{row_id}'.encode(), pickle.dumps(processed_record))
            
    env_src.close()
    env_dst.close()
    print(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {processed_db_path}")

if __name__ == "__main__":
    main("configs/predictor_fusion.yaml")