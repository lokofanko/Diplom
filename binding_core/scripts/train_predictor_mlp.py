import sys
import os
import yaml
import argparse
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from torch.utils.data import DataLoader
from tqdm import tqdm
from clearml import Task
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# –ü—É—Ç—å –∫ src
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.utils.dataset import EmbeddingLMDBDataset

# --- –ú–û–î–ï–õ–¨ MLP ---
class BindingPredictorMLP(nn.Module):
    def __init__(self, input_dim=1792, hidden_dim=1024, dropout=0.2):
        super().__init__()
        
        # –¢—Ä–µ—Ö—Å–ª–æ–π–Ω—ã–π –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω —Å BatchNorm –∏ Dropout
        self.net = nn.Sequential(
            # –°–ª–æ–π 1
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            
            # –°–ª–æ–π 2
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            
            # –°–ª–æ–π 3 (–í—ã—Ö–æ–¥)
            nn.Linear(hidden_dim // 2, 1)
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Kaiming
        for m in self.net:
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None: nn.init.constant_(m.bias, 0)
        
    def forward(self, x):
        return self.net(x).squeeze()

def run_epoch(model, loader, criterion, optimizer, device, is_train):
    model.train(is_train)
    total_loss = 0.0
    all_preds = []
    all_targets = []
    
    context = torch.enable_grad() if is_train else torch.no_grad()
    
    with context:
        for batch in tqdm(loader, desc="Train" if is_train else "Val", leave=False):
            inputs = batch['embedding'].to(device)
            targets = batch['target'].to(device)
            
            if is_train:
                optimizer.zero_grad()
                preds = model(inputs)
                loss = criterion(preds, targets)
                loss.backward()
                optimizer.step()
            else:
                preds = model(inputs)
                loss = criterion(preds, targets)
            
            total_loss += loss.item()
            all_preds.extend(preds.detach().cpu().numpy())
            all_targets.extend(targets.detach().cpu().numpy())
            
    avg_loss = total_loss / len(loader)
    mae = mean_absolute_error(all_targets, all_preds)
    rmse = np.sqrt(mean_squared_error(all_targets, all_preds))
    
    return avg_loss, mae, rmse

def evaluate_and_plot(model, loader, device, name, save_dir, logger):
    """
    –ü—Ä–æ–≥–æ–Ω—è–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å, —Å—á–∏—Ç–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏ —Ä–∏—Å—É–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏.
    """
    model.eval()
    preds, targets = [], []
    
    with torch.no_grad():
        for batch in tqdm(loader, desc=f"Evaluating {name}"):
            X = batch['embedding'].to(device)
            y = batch['target'].to(device)
            p = model(X)
            preds.extend(p.cpu().numpy())
            targets.extend(y.cpu().numpy())
            
    preds = np.array(preds)
    targets = np.array(targets)
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    mae = mean_absolute_error(targets, preds)
    r2 = r2_score(targets, preds)
    mask = targets != 0
    mape = np.mean(np.abs((targets[mask] - preds[mask]) / targets[mask])) * 100
    
    print(f"   üèÜ {name} Results: MAE={mae:.4f}, R¬≤={r2:.4f}, MAPE={mape:.2f}%")
    
    # –ì—Ä–∞—Ñ–∏–∫–∏
    plt.figure(figsize=(14, 6))
    
    # Scatter
    plt.subplot(1, 2, 1)
    lo, hi = min(targets.min(), preds.min()), max(targets.max(), preds.max())
    plt.plot([lo, hi], [lo, hi], 'r--', lw=2)
    sns.scatterplot(x=targets, y=preds, alpha=0.3, s=15)
    plt.xlabel("Actual pIC50")
    plt.ylabel("Predicted pIC50")
    plt.title(f"{name} Set: Pred vs True\nR¬≤ = {r2:.3f}")
    plt.grid(True, alpha=0.3)
    
    # Hist
    plt.subplot(1, 2, 2)
    err = targets - preds
    sns.histplot(err, bins=40, kde=True, color='purple')
    plt.axvline(0, color='red', linestyle='--')
    plt.xlabel("Error (Actual - Pred)")
    plt.title(f"{name} Error Distribution\nMAE = {mae:.3f}")
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plot_path = os.path.join(save_dir, f"results_{name.lower()}.png")
    plt.savefig(plot_path, dpi=300)
    plt.close()
    
    print(f"      Saved plot: {plot_path}")
    
    # ClearML Log
    if logger:
        logger.report_image(f"Results {name}", "Plots", local_path=plot_path)

def main(config_path):
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞
    with open(config_path, 'r') as f:
        cfg = yaml.safe_load(f)

    # 2. ClearML Init
    task = Task.init(
        project_name=cfg['project_name'], 
        task_name=cfg['experiment_name']
    )
    task.connect(cfg)
    logger = task.get_logger()

    device = torch.device(cfg['training']['device'])
    print(f"üöÄ Device: {device}")

    # 3. –î–∞–Ω–Ω—ã–µ
    emb_db_path = cfg['paths']['embeddings_db_path']
    splits_dir = cfg['paths']['splits_dir']
    
    print(f"üíø Loading from: {emb_db_path}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (–∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ, –µ—Å–ª–∏ –≤ –∫–æ–Ω—Ñ–∏–≥–µ –Ω–µ—Ç)
    train_file = cfg['paths'].get('train_indices', 'train_indices.csv')
    val_file = cfg['paths'].get('val_indices', 'val_indices.csv')
    test_file = cfg['paths'].get('test_indices', 'test_indices.csv')

    train_ds = EmbeddingLMDBDataset(emb_db_path, os.path.join(splits_dir, train_file))
    val_ds = EmbeddingLMDBDataset(emb_db_path, os.path.join(splits_dir, val_file))
    test_ds = EmbeddingLMDBDataset(emb_db_path, os.path.join(splits_dir, test_file))
    
    kwargs = {
        'batch_size': cfg['training']['batch_size'],
        'num_workers': cfg['training'].get('num_workers', 4),
        'pin_memory': True
    }
    
    train_loader = DataLoader(train_ds, shuffle=True, **kwargs)
    val_loader = DataLoader(val_ds, shuffle=False, **kwargs)
    # –î–ª—è —Ç–µ—Å—Ç–∞ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ—Ç –∂–µ –±–∞—Ç—á
    test_loader = DataLoader(test_ds, shuffle=False, **kwargs)

    # 4. –ú–æ–¥–µ–ª—å
    print("üß† Building MLP...")
    model = BindingPredictorMLP(
        input_dim=cfg['model']['input_dim'],
        hidden_dim=cfg['model']['hidden_dim'],
        dropout=cfg['model']['dropout']
    ).to(device)

    lr_val = float(cfg['training']['learning_rate'])
    wd_val = float(cfg['training']['weight_decay'])

    optimizer = torch.optim.AdamW(model.parameters(), lr=lr_val, weight_decay=wd_val)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)
    criterion = nn.MSELoss()

    # 5. Loop
    best_val_mae = float('inf')
    early_stop_counter = 0
    exp_dir = os.path.join(cfg['paths']['save_dir'], cfg['experiment_name'])
    os.makedirs(exp_dir, exist_ok=True)
    
    print(f"üî• Training for {cfg['training']['epochs']} epochs...")
    
    # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä early_stopping –∏–ª–∏ early_stopping_patience
    patience = cfg['training'].get('early_stopping', cfg['training'].get('early_stopping_patience', 5))

    for epoch in range(cfg['training']['epochs']):
        # Train
        t_loss, t_mae, t_rmse = run_epoch(model, train_loader, criterion, optimizer, device, True)
        # Val
        v_loss, v_mae, v_rmse = run_epoch(model, val_loader, criterion, None, device, False)
        
        # Scheduler Step
        scheduler.step(v_loss)

        # Logs
        print(f"Epoch {epoch+1:03d} | Train MAE: {t_mae:.4f} | Val MAE: {v_mae:.4f} | Val RMSE: {v_rmse:.4f}")
        logger.report_scalar("MAE", "Train", iteration=epoch, value=t_mae)
        logger.report_scalar("MAE", "Val", iteration=epoch, value=v_mae)
        logger.report_scalar("Loss", "Val", iteration=epoch, value=v_loss)

        # Save Best
        if v_mae < best_val_mae:
            best_val_mae = v_mae
            early_stop_counter = 0
            torch.save(model.state_dict(), os.path.join(exp_dir, "best_model.pt"))
            print(f"  üíæ New Best Model! (MAE: {best_val_mae:.4f})")
        else:
            early_stop_counter += 1
            if early_stop_counter >= patience:
                print("üõë Early Stopping!")
                break
    
    print("üèÜ Training Finished. Best Validation MAE:", best_val_mae)

    # 6. Final Evaluation & Plots
    print("\nüìä Generating Final Reports...")
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
    model.load_state_dict(torch.load(os.path.join(exp_dir, "best_model.pt")))
    
    # –†–∏—Å—É–µ–º –¥–ª—è –≤—Å–µ—Ö —Å–ø–ª–∏—Ç–æ–≤
    evaluate_and_plot(model, train_loader, device, "Train", exp_dir, logger)
    evaluate_and_plot(model, val_loader, device, "Val", exp_dir, logger)
    evaluate_and_plot(model, test_loader, device, "Test", exp_dir, logger)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="configs/predictor_mlp.yaml")
    args = parser.parse_args()
    main(args.config)
