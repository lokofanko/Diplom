import torch
import torch.nn as nn
from transformers import AutoModel, AutoConfig
from pathlib import Path

class BindingPredictor(nn.Module):
    def __init__(self, 
                 base_model_dir='models/pretrained',
                 hidden_dim=256, 
                 dropout=0.1,
                 freeze_encoders=True): # <--- –ì–õ–ê–í–ù–´–ô –°–ü–ê–°–ò–¢–ï–õ–¨ –ü–ê–ú–Ø–¢–ò
        super().__init__()
        
        print("üèóÔ∏è  –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BindingPredictor...")
        
        # –ü—É—Ç–∏ –∫ –≤–µ—Å–∞–º
        prot_path = Path(base_model_dir) / 'protbert'
        mol_path = Path(base_model_dir) / 'chemberta'
        
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ ProtBERT
        print(f"    –ó–∞–≥—Ä—É–∑–∫–∞ ProtBERT –∏–∑ {prot_path}...")
        try:
            self.prot_bert = AutoModel.from_pretrained(prot_path)
        except OSError:
            raise OSError(f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã –≤–µ—Å–∞ –≤ {prot_path}. –ó–∞–ø—É—Å—Ç–∏ download_models.py!")

        self.prot_hidden = self.prot_bert.config.hidden_size # 1024
        
        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ ChemBERTa
        print(f"    –ó–∞–≥—Ä—É–∑–∫–∞ ChemBERTa –∏–∑ {mol_path}...")
        try:
            self.mol_bert = AutoModel.from_pretrained(mol_path)
        except OSError:
            raise OSError(f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã –≤–µ—Å–∞ –≤ {mol_path}. –ó–∞–ø—É—Å—Ç–∏ download_models.py!")
            
        self.mol_hidden = self.mol_bert.config.hidden_size # 768
        
        # 3. –ó–ê–ú–û–†–û–ó–ö–ê (FREEZING)
        # –ï—Å–ª–∏ –ø–∞–º—è—Ç–∏ –º–∞–ª–æ, –º—ã –æ—Ç–∫–ª—é—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤.
        # –û–Ω–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –ø—Ä–æ—Å—Ç–æ –∫–∞–∫ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
        if freeze_encoders:
            print("    ‚ùÑÔ∏è  –†–ï–ñ–ò–ú –ó–ê–ú–û–†–û–ó–ö–ò: –í–µ—Å–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –Ω–µ –±—É–¥—É—Ç (—ç–∫–æ–Ω–æ–º–∏—è RAM).")
            for param in self.prot_bert.parameters():
                param.requires_grad = False
            for param in self.mol_bert.parameters():
                param.requires_grad = False
        else:
            print("    üî•  –†–ï–ñ–ò–ú FINE-TUNING: –û—Å—Ç–æ—Ä–æ–∂–Ω–æ, –∂—Ä–µ—Ç –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏!")

        # 4. Fusion Head (–ì–æ–ª–æ–≤–∞)
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–∞ (1024 + 768 = 1792) -> –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ 1 —á–∏—Å–ª–æ
        combined_dim = self.prot_hidden + self.mol_hidden
        
        self.head = nn.Sequential(
            nn.Linear(combined_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            
            nn.Linear(hidden_dim // 2, 1)
        )
        
    def forward(self, prot_input, mol_input):
        # prot_input –∏ mol_input - —ç—Ç–æ —Å–ª–æ–≤–∞—Ä–∏ {'input_ids': ..., 'attention_mask': ...}
        
        # A. –ë–µ–ª–æ–∫
        # –ü—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ BERT
        prot_out = self.prot_bert(**prot_input)
        
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥. –î–ª—è ProtBERT –ª—É—á—à–µ –¥–µ–ª–∞—Ç—å Mean Pooling (—É—Å–ø—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø–æ –¥–ª–∏–Ω–µ),
        # —Ç–∞–∫ –∫–∞–∫ CLS —Ç–æ–∫–µ–Ω –∏–Ω–æ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö—É–∂–µ –¥–ª—è –∑–∞–¥–∞—á —Ä–µ–≥—Ä–µ—Å—Å–∏–∏.
        # –ú–∞—Å–∫–∞ –Ω—É–∂–Ω–∞, —á—Ç–æ–±—ã –Ω–µ —É—Å—Ä–µ–¥–Ω—è—Ç—å –ø–∞–¥–¥–∏–Ω–≥–∏ (–ø—É—Å—Ç—ã–µ —Ö–≤–æ—Å—Ç—ã).
        p_mask = prot_input['attention_mask'].unsqueeze(-1).expand(prot_out.last_hidden_state.size()).float()
        p_emb = torch.sum(prot_out.last_hidden_state * p_mask, 1) / torch.clamp(p_mask.sum(1), min=1e-9)

        # B. –ú–æ–ª–µ–∫—É–ª–∞
        mol_out = self.mol_bert(**mol_input)
        # –î–ª—è ChemBERTa —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç CLS —Ç–æ–∫–µ–Ω (pooler_output), –Ω–æ Mean Pooling —Ç–æ–∂–µ –Ω–∞–¥–µ–∂–µ–Ω.
        # –í–æ–∑—å–º–µ–º Mean Pooling –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏.
        m_mask = mol_input['attention_mask'].unsqueeze(-1).expand(mol_out.last_hidden_state.size()).float()
        m_emb = torch.sum(mol_out.last_hidden_state * m_mask, 1) / torch.clamp(m_mask.sum(1), min=1e-9)
        
        # C. –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è
        combined = torch.cat((p_emb, m_emb), dim=1)
        
        # D. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        output = self.head(combined)
        
        return output.squeeze()

if __name__ == "__main__":
    # –¢–µ—Å—Ç –Ω–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å
    model = BindingPredictor(freeze_encoders=True)
    print("‚úÖ –¢–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω: –º–æ–¥–µ–ª—å —Å–æ–±—Ä–∞–ª–∞—Å—å.")