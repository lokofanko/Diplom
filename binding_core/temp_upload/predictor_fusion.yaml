experiment_name: "Fusion_DistilProtBERT_NoWorkers_Test"
project_name: "MSU-Baseline"

paths:
  db_path: "../data/baseline/db"
  processed_db_path: "../data/baseline/db_processed"
  splits_dir: "../data/baseline/splits"
  # ⬇️ ИЗМЕНЕНИЕ: Используем DistilProtBERT
  prot_model_path: "../models/pretrained/distilprotbert" 
  mol_model_path: "../models/pretrained/chemberta"
  save_dir: "experiments/"

data:
  exclude_mutants: True 
  mol_max_len: 128
  prot_max_len: 128 # <--- ОЧЕНЬ АГРЕССИВНО УМЕНЬШИЛИ ДЛИНУ (было 256)
  
model:
  hidden_dim: 256
  dropout: 0.1
  freeze_encoders: True
  
training:
  batch_size: 16       # <--- УМЕНЬШИМ ЕЩЕ РАЗ (было 32)
  num_workers: 0       # <--- ГЛАВНОЕ ИЗМЕНЕНИЕ: ОТКЛЮЧАЕМ МУЛЬТИПРОЦЕССИНГ
  learning_rate: 0.001 
  epochs: 2            
  device: "cuda"       
  log_step_interval: 10 # Логируем каждые 10 шагов для быстрого фидбека
  use_amp: True 